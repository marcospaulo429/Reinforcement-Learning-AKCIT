{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste Behavior Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting episode 1\n",
      "Collecting episode 2\n",
      "Collecting episode 3\n",
      "Collecting episode 4\n",
      "Collecting episode 5\n",
      "Collected 5 episodes.\n",
      "[Behavior] Epoch 1/100 | Value Loss: 0.5667 | Actor Loss: 0.0662\n",
      "[Behavior] Epoch 2/100 | Value Loss: 0.5720 | Actor Loss: 0.1371\n",
      "[Behavior] Epoch 3/100 | Value Loss: 0.5653 | Actor Loss: 0.2113\n",
      "[Behavior] Epoch 4/100 | Value Loss: 0.5506 | Actor Loss: 0.2897\n",
      "[Behavior] Epoch 5/100 | Value Loss: 0.5503 | Actor Loss: 0.3791\n",
      "[Behavior] Epoch 6/100 | Value Loss: 0.5736 | Actor Loss: 0.4829\n",
      "[Behavior] Epoch 7/100 | Value Loss: 0.5790 | Actor Loss: 0.6029\n",
      "[Behavior] Epoch 8/100 | Value Loss: 0.5991 | Actor Loss: 0.7457\n",
      "[Behavior] Epoch 9/100 | Value Loss: 0.6023 | Actor Loss: 0.9146\n",
      "[Behavior] Epoch 10/100 | Value Loss: 0.6456 | Actor Loss: 1.1084\n",
      "[Behavior] Epoch 11/100 | Value Loss: 0.6612 | Actor Loss: 1.3286\n",
      "[Behavior] Epoch 12/100 | Value Loss: 0.6998 | Actor Loss: 1.5756\n",
      "[Behavior] Epoch 13/100 | Value Loss: 0.7410 | Actor Loss: 1.8517\n",
      "[Behavior] Epoch 14/100 | Value Loss: 0.7340 | Actor Loss: 2.1446\n",
      "[Behavior] Epoch 15/100 | Value Loss: 0.7070 | Actor Loss: 2.4557\n",
      "[Behavior] Epoch 16/100 | Value Loss: 0.6562 | Actor Loss: 2.8020\n",
      "[Behavior] Epoch 17/100 | Value Loss: 0.6153 | Actor Loss: 3.1817\n",
      "[Behavior] Epoch 18/100 | Value Loss: 0.5931 | Actor Loss: 3.6071\n",
      "[Behavior] Epoch 19/100 | Value Loss: 0.5648 | Actor Loss: 4.0822\n",
      "[Behavior] Epoch 20/100 | Value Loss: 0.5667 | Actor Loss: 4.5901\n",
      "[Behavior] Epoch 21/100 | Value Loss: 0.5761 | Actor Loss: 5.1204\n",
      "[Behavior] Epoch 22/100 | Value Loss: 0.6050 | Actor Loss: 5.6557\n",
      "[Behavior] Epoch 23/100 | Value Loss: 0.6474 | Actor Loss: 6.1948\n",
      "[Behavior] Epoch 24/100 | Value Loss: 0.6869 | Actor Loss: 6.7024\n",
      "[Behavior] Epoch 25/100 | Value Loss: 0.6972 | Actor Loss: 7.1530\n",
      "[Behavior] Epoch 26/100 | Value Loss: 0.7484 | Actor Loss: 7.5160\n",
      "[Behavior] Epoch 27/100 | Value Loss: 0.7525 | Actor Loss: 7.8113\n",
      "[Behavior] Epoch 28/100 | Value Loss: 0.7751 | Actor Loss: 8.0222\n",
      "[Behavior] Epoch 29/100 | Value Loss: 0.7850 | Actor Loss: 8.1336\n",
      "[Behavior] Epoch 30/100 | Value Loss: 0.7848 | Actor Loss: 8.1980\n",
      "[Behavior] Epoch 31/100 | Value Loss: 0.7791 | Actor Loss: 8.2317\n",
      "[Behavior] Epoch 32/100 | Value Loss: 0.7742 | Actor Loss: 8.2446\n",
      "[Behavior] Epoch 33/100 | Value Loss: 0.7609 | Actor Loss: 8.2387\n",
      "[Behavior] Epoch 34/100 | Value Loss: 0.7583 | Actor Loss: 8.2219\n",
      "[Behavior] Epoch 35/100 | Value Loss: 0.7503 | Actor Loss: 8.2019\n",
      "[Behavior] Epoch 36/100 | Value Loss: 0.7338 | Actor Loss: 8.1721\n",
      "[Behavior] Epoch 37/100 | Value Loss: 0.7018 | Actor Loss: 8.1216\n",
      "[Behavior] Epoch 38/100 | Value Loss: 0.7040 | Actor Loss: 8.1056\n",
      "[Behavior] Epoch 39/100 | Value Loss: 0.6969 | Actor Loss: 8.1285\n",
      "[Behavior] Epoch 40/100 | Value Loss: 0.6953 | Actor Loss: 8.1399\n",
      "[Behavior] Epoch 41/100 | Value Loss: 0.6827 | Actor Loss: 8.1388\n",
      "[Behavior] Epoch 42/100 | Value Loss: 0.6696 | Actor Loss: 8.1241\n",
      "[Behavior] Epoch 43/100 | Value Loss: 0.6690 | Actor Loss: 8.1396\n",
      "[Behavior] Epoch 44/100 | Value Loss: 0.6637 | Actor Loss: 8.1554\n",
      "[Behavior] Epoch 45/100 | Value Loss: 0.6596 | Actor Loss: 8.1553\n",
      "[Behavior] Epoch 46/100 | Value Loss: 0.6563 | Actor Loss: 8.1578\n",
      "[Behavior] Epoch 47/100 | Value Loss: 0.6521 | Actor Loss: 8.1519\n",
      "[Behavior] Epoch 48/100 | Value Loss: 0.6496 | Actor Loss: 8.1850\n",
      "[Behavior] Epoch 49/100 | Value Loss: 0.6510 | Actor Loss: 8.2240\n",
      "[Behavior] Epoch 50/100 | Value Loss: 0.6358 | Actor Loss: 8.2185\n",
      "[Behavior] Epoch 51/100 | Value Loss: 0.6276 | Actor Loss: 8.2323\n",
      "[Behavior] Epoch 52/100 | Value Loss: 0.6200 | Actor Loss: 8.2485\n",
      "[Behavior] Epoch 53/100 | Value Loss: 0.6177 | Actor Loss: 8.2605\n",
      "[Behavior] Epoch 54/100 | Value Loss: 0.6092 | Actor Loss: 8.2742\n",
      "[Behavior] Epoch 55/100 | Value Loss: 0.6064 | Actor Loss: 8.2870\n",
      "[Behavior] Epoch 56/100 | Value Loss: 0.5966 | Actor Loss: 8.2832\n",
      "[Behavior] Epoch 57/100 | Value Loss: 0.5902 | Actor Loss: 8.2776\n",
      "[Behavior] Epoch 58/100 | Value Loss: 0.5831 | Actor Loss: 8.2651\n",
      "[Behavior] Epoch 59/100 | Value Loss: 0.5851 | Actor Loss: 8.2648\n",
      "[Behavior] Epoch 60/100 | Value Loss: 0.5894 | Actor Loss: 8.2641\n",
      "[Behavior] Epoch 61/100 | Value Loss: 0.5859 | Actor Loss: 8.2811\n",
      "[Behavior] Epoch 62/100 | Value Loss: 0.5768 | Actor Loss: 8.3035\n",
      "[Behavior] Epoch 63/100 | Value Loss: 0.5770 | Actor Loss: 8.3239\n",
      "[Behavior] Epoch 64/100 | Value Loss: 0.5695 | Actor Loss: 8.3548\n",
      "[Behavior] Epoch 65/100 | Value Loss: 0.5769 | Actor Loss: 8.3796\n",
      "[Behavior] Epoch 66/100 | Value Loss: 0.5651 | Actor Loss: 8.3684\n",
      "[Behavior] Epoch 67/100 | Value Loss: 0.5567 | Actor Loss: 8.3721\n",
      "[Behavior] Epoch 68/100 | Value Loss: 0.5402 | Actor Loss: 8.3837\n",
      "[Behavior] Epoch 69/100 | Value Loss: 0.5393 | Actor Loss: 8.3875\n",
      "[Behavior] Epoch 70/100 | Value Loss: 0.5378 | Actor Loss: 8.3860\n",
      "[Behavior] Epoch 71/100 | Value Loss: 0.5421 | Actor Loss: 8.3839\n",
      "[Behavior] Epoch 72/100 | Value Loss: 0.5366 | Actor Loss: 8.3843\n",
      "[Behavior] Epoch 73/100 | Value Loss: 0.5442 | Actor Loss: 8.4174\n",
      "[Behavior] Epoch 74/100 | Value Loss: 0.5383 | Actor Loss: 8.4323\n",
      "[Behavior] Epoch 75/100 | Value Loss: 0.5275 | Actor Loss: 8.4065\n",
      "[Behavior] Epoch 76/100 | Value Loss: 0.5201 | Actor Loss: 8.3618\n",
      "[Behavior] Epoch 77/100 | Value Loss: 0.5178 | Actor Loss: 8.3207\n",
      "[Behavior] Epoch 78/100 | Value Loss: 0.5124 | Actor Loss: 8.2945\n",
      "[Behavior] Epoch 79/100 | Value Loss: 0.5094 | Actor Loss: 8.3020\n",
      "[Behavior] Epoch 80/100 | Value Loss: 0.4911 | Actor Loss: 8.2949\n",
      "[Behavior] Epoch 81/100 | Value Loss: 0.4877 | Actor Loss: 8.2944\n",
      "[Behavior] Epoch 82/100 | Value Loss: 0.4960 | Actor Loss: 8.3117\n",
      "[Behavior] Epoch 83/100 | Value Loss: 0.4871 | Actor Loss: 8.3228\n",
      "[Behavior] Epoch 84/100 | Value Loss: 0.4922 | Actor Loss: 8.3060\n",
      "[Behavior] Epoch 85/100 | Value Loss: 0.4742 | Actor Loss: 8.2858\n",
      "[Behavior] Epoch 86/100 | Value Loss: 0.4767 | Actor Loss: 8.2739\n",
      "[Behavior] Epoch 87/100 | Value Loss: 0.4738 | Actor Loss: 8.2631\n",
      "[Behavior] Epoch 88/100 | Value Loss: 0.4712 | Actor Loss: 8.2719\n",
      "[Behavior] Epoch 89/100 | Value Loss: 0.4619 | Actor Loss: 8.2951\n",
      "[Behavior] Epoch 90/100 | Value Loss: 0.4601 | Actor Loss: 8.3432\n",
      "[Behavior] Epoch 91/100 | Value Loss: 0.4545 | Actor Loss: 8.3692\n",
      "[Behavior] Epoch 92/100 | Value Loss: 0.4672 | Actor Loss: 8.3973\n",
      "[Behavior] Epoch 93/100 | Value Loss: 0.4686 | Actor Loss: 8.4258\n",
      "[Behavior] Epoch 94/100 | Value Loss: 0.4557 | Actor Loss: 8.4424\n",
      "[Behavior] Epoch 95/100 | Value Loss: 0.4518 | Actor Loss: 8.4206\n",
      "[Behavior] Epoch 96/100 | Value Loss: 0.4567 | Actor Loss: 8.4011\n",
      "[Behavior] Epoch 97/100 | Value Loss: 0.4452 | Actor Loss: 8.3856\n",
      "[Behavior] Epoch 98/100 | Value Loss: 0.4477 | Actor Loss: 8.3905\n",
      "[Behavior] Epoch 99/100 | Value Loss: 0.4395 | Actor Loss: 8.3964\n",
      "[Behavior] Epoch 100/100 | Value Loss: 0.4397 | Actor Loss: 8.4041\n",
      "Behavior learning test completed.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dm_control import suite\n",
    "from dm_control.suite.wrappers import pixels\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from torch.utils.data import DataLoader\n",
    "from replay_buffer import ReplayBuffer\n",
    "from actor_critic import ActionModel, ValueNet\n",
    "from general_algo import collect_replay_buffer\n",
    "from auxiliares import converter_cinza, training_device\n",
    "from train_wm import DreamerWorldModel  # Importa sua implementação do DreamerWorldModel\n",
    "from behavior_learning import extract_latent_sequences, create_latent_dataset, behavior_learning  # Ajuste o nome do módulo se necessário\n",
    "\n",
    "# Parâmetros e dispositivo\n",
    "S = 5  # Número de episódios a coletar\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Cria o ambiente\n",
    "env = suite.load(domain_name=\"cartpole\", task_name=\"swingup\")\n",
    "env = pixels.Wrapper(env, pixels_only=True,\n",
    "                        render_kwargs={'height': 84, 'width': 84, 'camera_id': 0})\n",
    "\n",
    "replay_buffer = collect_replay_buffer(env,S,ReplayBuffer())\n",
    "\n",
    "# Instancia o world_model, ator e value_net\n",
    "input_size = None  # Não utilizado pelo autoencoder\n",
    "latent_dim = 256\n",
    "action_dim = np.prod(env.action_spec().shape)\n",
    "hidden_dim = 256\n",
    "\n",
    "world_model = DreamerWorldModel(input_size, latent_dim, action_dim, hidden_dim).to(device)\n",
    "actor = ActionModel(latent_dim, action_dim).to(device)\n",
    "value_net = ValueNet(latent_dim).to(device)\n",
    "\n",
    "# Extrai sequências latentes usando o world_model treinado\n",
    "latent_buffer = extract_latent_sequences(world_model, replay_buffer, device)\n",
    "\n",
    "# Cria o dataset latente e o DataLoader\n",
    "latent_dataset = create_latent_dataset(latent_buffer)\n",
    "latent_loader = DataLoader(latent_dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "# Define otimizadores e função de perda\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=8e-5)\n",
    "value_optimizer = optim.Adam(value_net.parameters(), lr=8e-5)\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Executa o Behavior Learning com rollouts imaginados\n",
    "epochs_behavior = 100\n",
    "actor, value_net, actor_loss_history, value_loss_history = behavior_learning(\n",
    "    world_model, actor, value_net,\n",
    "    latent_loader,\n",
    "    device,\n",
    "    horizon=15,\n",
    "    gamma=0.99,\n",
    "    lam=0.95,\n",
    "    value_optimizer=value_optimizer,\n",
    "    actor_optimizer=actor_optimizer,\n",
    "    mse_loss=mse_loss,\n",
    "    epochs_behavior=epochs_behavior\n",
    ")\n",
    "\n",
    "print(\"Behavior learning test completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcospaulo2\u001b[0m (\u001b[33mmarcospaulo2-federal-university-of-goi-s\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/marcospaulo/Documents/CEIA/Reforço/Implementações/Reinforcement-Learning-AKCIT/DreamerV1/wandb/run-20250312_224048-c9htnyik</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcospaulo2-federal-university-of-goi-s/teste_custom_steps/runs/c9htnyik' target=\"_blank\">teste_custom_steps_run</a></strong> to <a href='https://wandb.ai/marcospaulo2-federal-university-of-goi-s/teste_custom_steps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcospaulo2-federal-university-of-goi-s/teste_custom_steps' target=\"_blank\">https://wandb.ai/marcospaulo2-federal-university-of-goi-s/teste_custom_steps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcospaulo2-federal-university-of-goi-s/teste_custom_steps/runs/c9htnyik' target=\"_blank\">https://wandb.ai/marcospaulo2-federal-university-of-goi-s/teste_custom_steps/runs/c9htnyik</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step 1: Reward = 40 | Inner step 10: Inner Metric = 5.0\n",
      "Global step 2: Reward = 80 | Inner step 20: Inner Metric = 10.0\n",
      "Global step 3: Reward = 120 | Inner step 30: Inner Metric = 15.0\n",
      "Global step 4: Reward = 160 | Inner step 40: Inner Metric = 20.0\n",
      "Global step 5: Reward = 200 | Inner step 50: Inner Metric = 25.0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Inner/Metric</td><td>▁▃▅▆█</td></tr><tr><td>Reward/Episode</td><td>▁▃▅▆█</td></tr><tr><td>global_step</td><td>▁▃▅▆█</td></tr><tr><td>inner_step</td><td>▁▃▅▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Inner/Metric</td><td>25</td></tr><tr><td>global_step</td><td>5</td></tr><tr><td>inner_step</td><td>50</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">teste_custom_steps_run</strong> at: <a href='https://wandb.ai/marcospaulo2-federal-university-of-goi-s/teste_custom_steps/runs/c9htnyik' target=\"_blank\">https://wandb.ai/marcospaulo2-federal-university-of-goi-s/teste_custom_steps/runs/c9htnyik</a><br> View project at: <a href='https://wandb.ai/marcospaulo2-federal-university-of-goi-s/teste_custom_steps' target=\"_blank\">https://wandb.ai/marcospaulo2-federal-university-of-goi-s/teste_custom_steps</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250312_224048-c9htnyik/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import wandb\n",
    "\n",
    "# Inicializa o wandb\n",
    "wandb.init(project=\"teste_custom_steps\", name=\"teste_custom_steps_run\")\n",
    "\n",
    "# Define as métricas com seus step_metrics personalizados\n",
    "wandb.define_metric(\"Reward/Episode\", step_metric=\"global_step\", summary=\"max\")\n",
    "wandb.define_metric(\"Inner/Metric\", step_metric=\"inner_step\", step_sync=True)\n",
    "\n",
    "global_step = 0  # Contador para o loop externo\n",
    "inner_step = 0   # Contador para o loop interno\n",
    "\n",
    "for i in range(5):\n",
    "    global_step += 1\n",
    "    # Simula um valor de reward para o loop externo\n",
    "    reward = global_step * 40\n",
    "\n",
    "    # No loop interno simulamos um contador com granularidade diferente\n",
    "    inner_step += 10  # Por exemplo, a cada iteração do loop externo, o inner_step aumenta em 3\n",
    "    inner_metric = inner_step * 0.5  # Valor dummy para inner_metric\n",
    "\n",
    "    # Registra ambas as métricas em uma única chamada, incluindo seus respectivos steps\n",
    "    wandb.log({\n",
    "        \"Reward/Episode\": reward,\n",
    "        \"global_step\": global_step,\n",
    "        \"Inner/Metric\": inner_metric,\n",
    "        \"inner_step\": inner_step\n",
    "    }, step=global_step)  # Aqui o step passado é o global_step\n",
    "\n",
    "    print(f\"Global step {global_step}: Reward = {reward} | Inner step {inner_step}: Inner Metric = {inner_metric}\")\n",
    "\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreamer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
