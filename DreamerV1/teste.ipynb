{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 120\u001b[0m\n\u001b[1;32m    117\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 57\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m decoder \u001b[38;5;241m=\u001b[39m ConvDecoder()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Coleta um episódio.\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m images_np, actions_np \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColetado episódio com \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(images_np)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m frames.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Pré-processa as imagens:\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# - Converte para tensor\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# - Normaliza para [0,1] e converte de (H, W, C) para (C, H, W)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m, in \u001b[0;36mcollect_episode\u001b[0;34m(env, max_steps)\u001b[0m\n\u001b[1;32m     24\u001b[0m     img \u001b[38;5;241m=\u001b[39m obs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Se a observação for um dicionário com 'image'\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     28\u001b[0m images\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Ação aleatória\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dm_control import suite\n",
    "\n",
    "# Importe as classes definidas do seu world model.\n",
    "# Certifique-se de que o arquivo contendo essas classes (por exemplo, world_model.py)\n",
    "# esteja no seu path ou no mesmo diretório.\n",
    "from models import RSSM, ConvEncoder, ConvDecoder\n",
    "\n",
    "def random_policy(action_spec):\n",
    "    \"\"\"Gera uma ação aleatória conforme as especificações do ambiente.\"\"\"\n",
    "    return np.random.uniform(low=action_spec.minimum, high=action_spec.maximum, size=action_spec.shape)\n",
    "\n",
    "def collect_episode(env, max_steps=1000):\n",
    "    \"\"\"Coleta um episódio executando uma política aleatória no ambiente.\"\"\"\n",
    "    time_limit = 10  # limite de tempo em segundos (exemplo)\n",
    "    obs = env.reset()\n",
    "    images, actions = [], []\n",
    "    current_time = 0.0\n",
    "    while current_time < time_limit:\n",
    "        # Usamos a imagem do pixel; ajuste se a chave for diferente.\n",
    "        if 'pixels' in obs:\n",
    "            img = obs['pixels']\n",
    "        else:\n",
    "            # Se a observação for um dicionário com 'image'\n",
    "            img = obs['image']\n",
    "        images.append(img)\n",
    "        \n",
    "        # Ação aleatória\n",
    "        action = random_policy(env.action_spec())\n",
    "        actions.append(action)\n",
    "        \n",
    "        timestep = env.step(action)\n",
    "        obs = timestep.observation\n",
    "        current_time = env.physics.data.time\n",
    "        if timestep.last():\n",
    "            break\n",
    "\n",
    "    images = np.array(images)  # (T, H, W, C)\n",
    "    actions = np.array(actions)  # (T, action_dim)\n",
    "    return images, actions\n",
    "\n",
    "def main():\n",
    "    # Carrega o ambiente Cartpole Swing-Up.\n",
    "    env = suite.load(domain_name=\"cartpole\", task_name=\"swingup\")\n",
    "    \n",
    "    # Define dispositivo de execução.\n",
    "    device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    # Inicializa os modelos.\n",
    "    rssm = RSSM().to(device)\n",
    "    encoder = ConvEncoder().to(device)\n",
    "    decoder = ConvDecoder().to(device)\n",
    "    \n",
    "    # Coleta um episódio.\n",
    "    images_np, actions_np = collect_episode(env, max_steps=1000)\n",
    "    print(f\"Coletado episódio com {len(images_np)} frames.\")\n",
    "    \n",
    "    # Pré-processa as imagens:\n",
    "    # - Converte para tensor\n",
    "    # - Normaliza para [0,1] e converte de (H, W, C) para (C, H, W)\n",
    "    images = torch.tensor(images_np, dtype=torch.float32) / 255.0\n",
    "    images = images.permute(0, 3, 1, 2)  # (T, C, H, W)\n",
    "    \n",
    "    # Converte as ações para tensor.\n",
    "    actions = torch.tensor(actions_np, dtype=torch.float32)\n",
    "    \n",
    "    # Adiciona uma dimensão de batch (batch = 1).\n",
    "    images = images.unsqueeze(0)  # (1, T, C, H, W)\n",
    "    actions = actions.unsqueeze(0)  # (1, T, action_dim)\n",
    "    \n",
    "    # Codifica as imagens usando o ConvEncoder.\n",
    "    # Reorganiza para processar todos os frames de uma vez.\n",
    "    B, T, C, H, W = images.shape\n",
    "    images_flat = images.view(B * T, C, H, W)\n",
    "    embed_flat = encoder({'image': images_flat})\n",
    "    embed = embed_flat.view(B, T, -1)  # (1, T, embed_dim)\n",
    "    \n",
    "    # Executa o método observe do RSSM para obter os estados latentes (post e prior).\n",
    "    post, prior = rssm.observe(embed, actions)\n",
    "    \n",
    "    # Gera uma trajetória imaginada (apenas usando ações) a partir do estado inicial.\n",
    "    imagine_prior = rssm.imagine(actions)\n",
    "    \n",
    "    # Exibe as formas das distribuições latentes.\n",
    "    print(\"Forma da variável 'stoch' posterior:\", post['stoch'].shape)\n",
    "    print(\"Forma da variável 'stoch' imaginada:\", imagine_prior['stoch'].shape)\n",
    "    \n",
    "    # Reconstrói a primeira imagem a partir do primeiro estado posterior.\n",
    "    # Obtemos os features concatenando stoch e deter.\n",
    "    first_state = {k: post[k][:, 0] for k in post}\n",
    "    features = rssm.get_feat(first_state)\n",
    "    recon_dist = decoder(features)\n",
    "    recon_img = recon_dist.mean  # (batch, C, H, W)\n",
    "    \n",
    "    # Exibe as formas das imagens.\n",
    "    print(\"Forma da imagem original:\", images[0, 0].shape)\n",
    "    print(\"Forma da imagem reconstruída:\", recon_img.shape)\n",
    "    \n",
    "    # Converte para numpy e reorganiza para (H, W, C) para exibição.\n",
    "    orig_img = images[0, 0].permute(1, 2, 0).cpu().numpy()\n",
    "    recon_img_np = recon_img[0].permute(1, 2, 0).detach().cpu().numpy()\n",
    "    \n",
    "    # Plota a imagem original e a reconstruída.\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(orig_img)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(recon_img_np)\n",
    "    plt.title(\"Reconstruída\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_v(rewards, values, tau, k, gamma=0.99, lamb=0.95, t=0):\n",
    "    \"\"\"\n",
    "    v_lambda : (1,B)\n",
    "    \"\"\"\n",
    "    horizon, B = rewards.shape\n",
    "    h = min(tau + k, t + horizon - 1)\n",
    "    \n",
    "    v = torch.zeros(1, B, dtype=rewards.dtype, device=rewards.device)\n",
    "    \n",
    "    for n in range(tau, h - 1):\n",
    "        v = (gamma ** (n - tau)) * rewards[n] + (gamma ** (h - tau) * values[h]) + v\n",
    "    \n",
    "    return v\n",
    "\n",
    "def compute_v_lambda(rewards, values, tau, gamma=0.99, lamb=0.95):\n",
    "    horizon, B = rewards.shape\n",
    "    v_lambda = torch.zeros(1, B, dtype=rewards.dtype, device=rewards.device)\n",
    "    for n in range(1, horizon - 1):\n",
    "        v1 = (lamb ** (n - 1) * compute_v(rewards, values, tau, n))\n",
    "        v2 = (lamb ** (horizon - 1))\n",
    "        v3 = compute_v(rewards, values, tau, horizon)\n",
    "        v_lambda = v1 + v2 * v3 + v_lambda\n",
    "    v_lambda = (1 - lamb) * v_lambda\n",
    "    \n",
    "    print(v1,v_lambda)\n",
    "    return v_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "tensor([[18.2981, 22.6691, 32.6450]], device='mps:0') tensor([[3.7633, 5.2423, 7.5074]], device='mps:0')\n",
      "Valor computado v: tensor([[3.7633, 5.2423, 7.5074]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "from auxiliares import training_device\n",
    "import torch\n",
    "device = training_device()\n",
    "\n",
    "# Definindo horizon e batch size\n",
    "horizon = 4  # número de timesteps\n",
    "B = 3        # batch size\n",
    "\n",
    "# Cria um tensor de recompensas com valores de exemplo (dimensão: horizon x B)\n",
    "rewards = torch.tensor([\n",
    "    [7.5, 2.3, 3.0],\n",
    "    [0.5, 1.0, 1.5],\n",
    "    [0.2, 0.4, 0.6],\n",
    "    [0.1, 0.2, 0.3]\n",
    "], device=device)\n",
    "\n",
    "# Cria um tensor de valores com valores de exemplo (dimensão: horizon x B)\n",
    "values = torch.tensor([\n",
    "    [10.0, 20.0, 30.0],\n",
    "    [11.0, 21.0, 31.0],\n",
    "    [12.0, 22.0, 32.0],\n",
    "    [13.0, 23.0, 33.0]\n",
    "], device=device)\n",
    "\n",
    "# Parâmetros para a função compute_v\n",
    "tau = 0\n",
    "k = 2\n",
    "gamma = 0.99\n",
    "\n",
    "# Chama a função e imprime o resultado\n",
    "v = compute_v_lambda(rewards, values, tau, k)\n",
    "print(\"Valor computado v:\", v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreamer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
