{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6cb69c-cd9d-4e13-af90-77cd9fe6133e",
   "metadata": {},
   "source": [
    "Install the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf9741a-19aa-4ae9-bcbb-1b77897c0560",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 2686,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1732086835810,
    "lastExecutedByKernel": "773dcfbd-4203-4f1d-ae2a-2fdaf93cdf9b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install torch numpy matplotlib gym==0.25.2",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "#!pip install torch dm_control numpy==1.23.5 matplotlib gym==0.25.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28ca76d",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/tutorial/proximal-policy-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74521a42-002f-4005-9803-01074d8097a7",
   "metadata": {},
   "source": [
    "Import the packages in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6127570-70f4-4626-b9fa-a23852211c5a",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1732086835863,
    "lastExecutedByKernel": "773dcfbd-4203-4f1d-ae2a-2fdaf93cdf9b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as f\nimport torch.distributions as distributions\nfrom torch.utils.data import DataLoader, TensorDataset\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport gym\n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as f\n",
    "import torch.distributions as distributions\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "from dm_control import suite\n",
    "from dm_control.suite.wrappers import pixels\n",
    "from auxiliares import converter_cinza, training_device\n",
    "device = training_device()\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c859d8",
   "metadata": {},
   "source": [
    "Loading autoencoder for deepmind control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "891088ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(z_mean, z_log_var):\n",
    "    std = torch.exp(0.5 * z_log_var)\n",
    "    eps = torch.randn_like(std)\n",
    "    return z_mean + eps * std\n",
    "\n",
    "# Encoder com mais camadas convolucionais para maior poder representacional\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim, in_channels=1,hidden_units = 32):  \n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_units, kernel_size=4, stride=2, padding=1)   # -> (batch, 64, 42, 42)\n",
    "        self.conv2 = nn.Conv2d(hidden_units, hidden_units*2, kernel_size=4, stride=2, padding=1)            # -> (batch, 128, 21, 21)\n",
    "        self.conv3 = nn.Conv2d(hidden_units*2, hidden_units*4, kernel_size=4, stride=2, padding=1)           # -> (batch, 256, 10, 10)\n",
    "        \n",
    "        self.flatten_dim = hidden_units*4 * 10 * 10  # Tamanho do vetor achatado após as convoluções\n",
    "        self.fc_mean = nn.Linear(self.flatten_dim, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(self.flatten_dim, latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = f.relu(self.conv1(x))\n",
    "        x = f.relu(self.conv2(x))\n",
    "        x = f.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # Achata para (batch, flatten_dim)\n",
    "        z_mean = self.fc_mean(x)\n",
    "        z_log_var = self.fc_log_var(x)\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "class CNNDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, out_channels=1, hidden_units=32):  # Alterei o out_channels para 3 para imagens RGB\n",
    "        super(CNNDecoder, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.fc = nn.Linear(latent_dim, hidden_units*4 * 10 * 10)  # Mapeia o vetor latente para uma representação plana\n",
    "        self.deconv1 = nn.ConvTranspose2d(hidden_units*4, hidden_units*2, kernel_size=4, stride=2, padding=1, output_padding=1)  \n",
    "        self.deconv2 = nn.ConvTranspose2d(hidden_units*2, hidden_units, kernel_size=4, stride=2, padding=1, output_padding=0)\n",
    "        self.deconv3 = nn.ConvTranspose2d(hidden_units, out_channels, kernel_size=4, stride=2, padding=1, output_padding=0)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(x.size(0), (self.hidden_units)*4, 10, 10)  # Reorganiza para (batch, 256, 10, 10)\n",
    "        x = f.relu(self.deconv1(x))\n",
    "        x = f.relu(self.deconv2(x))\n",
    "        x = torch.tanh(self.deconv3(x))  # Sigmoid para valores entre 0 e 1\n",
    "        return x\n",
    "\n",
    "\n",
    "# VAE combinando o Encoder e o Decoder com CNNs mais complexas\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim, in_channels=1, hidden_units=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = CNNEncoder(latent_dim, in_channels,hidden_units=hidden_units)\n",
    "        self.decoder = CNNDecoder(latent_dim, out_channels=in_channels, hidden_units=hidden_units)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_mean, z_log_var = self.encoder(x)\n",
    "        z = reparameterize(z_mean, z_log_var)\n",
    "        recon = self.decoder(z)\n",
    "        return recon, z_mean, z_log_var\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3aa266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent_dim = 32\n",
    "hidden_units = 64\n",
    "batch_size = 2  # Pequeno para visualização fácil\n",
    "\n",
    "# Criar modelo\n",
    "vae = VAE(latent_dim=latent_dim, hidden_units=hidden_units).to(device)\n",
    "\n",
    "# Criar dados de teste fake (imagens 84x84 em preto e branco)\n",
    "test_input = torch.randn(batch_size, 1, 84, 84).to(device)  # Simula batch de 2 imagens\n",
    "\n",
    "print(\"\\n=== Testando Encoder ===\")\n",
    "z_mean, z_log_var = vae.encoder(test_input)\n",
    "print(f\"Saída do Encoder - z_mean shape: {z_mean.shape}\")\n",
    "print(f\"Saída do Encoder - z_log_var shape: {z_log_var.shape}\")\n",
    "\n",
    "print(\"\\n=== Testando Reparameterization ===\")\n",
    "z = reparameterize(z_mean, z_log_var)\n",
    "print(f\"z sample shape: {z.shape}\")\n",
    "\\\n",
    "print(\"\\n=== Testando Decoder ===\")\n",
    "recon = vae.decoder(z)\n",
    "print(f\"Reconstrução shape: {recon.shape} (deveria ser igual ao input {test_input.shape})\")\n",
    "\n",
    "print(\"\\n=== Teste Forward Completo ===\")\n",
    "recon, z_mean, z_log_var = vae(test_input)\n",
    "print(f\"Reconstrução shape: {recon.shape}\")\n",
    "print(f\"Média máxima na reconstrução: {recon.max().item():.4f}\")\n",
    "print(f\"Média mínima na reconstrução: {recon.min().item():.4f}\")\n",
    "\n",
    "# Verificação das dimensões das camadas\n",
    "print(\"\\n=== Verificação Dimensões ===\")\n",
    "print(\"Encoder:\")\n",
    "print(f\"conv1 weight shape: {vae.encoder.conv1.weight.shape}\")\n",
    "print(f\"conv2 weight shape: {vae.encoder.conv2.weight.shape}\")\n",
    "print(f\"conv3 weight shape: {vae.encoder.conv3.weight.shape}\")\n",
    "\n",
    "print(\"\\nDecoder:\")\n",
    "print(f\"deconv1 weight shape: {vae.decoder.deconv1.weight.shape}\")\n",
    "print(f\"deconv2 weight shape: {vae.decoder.deconv2.weight.shape}\")\n",
    "print(f\"deconv3 weight shape: {vae.decoder.deconv3.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b81aaa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint carregado com sucesso (Epoch 570)\n",
      "Loss de treino no checkpoint: 9.4301\n",
      "Loss de teste no checkpoint: 9.6787\n",
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "# 1. Defina os mesmos parâmetros usados no treinamento\n",
    "latent_dim = 128  # Substitua pelo valor usado originalmente\n",
    "in_channels = 1   # Substitua pelo valor usado originalmente\n",
    "hidden_units = 128 # Substitua pelo valor usado originalmente\n",
    "\n",
    "# 2. Instancie o modelo e otimizador (igual ao do treinamento)\n",
    "model = VAE(latent_dim=latent_dim, in_channels=in_channels, hidden_units=hidden_units).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())  # Ou o otimizador que você usou\n",
    "\n",
    "# 3. Caminho para o checkpoint\n",
    "checkpoint_dir = \"autoencoder/dgx_treino3-lat128_lr0.0001_h128/checkpoint_dgx_treino3-lat128_lr0.0001_h128_epoch570.pt\" #TODO\n",
    "epoch_to_load = 570                           # Epoch que deseja carregar\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_dir)\n",
    "\n",
    "# 4. Carregue o checkpoint\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device(device))\n",
    "    \n",
    "    # Carregue os estados\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Outras informações do checkpoint\n",
    "    loaded_epoch = checkpoint['epoch']\n",
    "    loaded_loss = checkpoint['loss']\n",
    "    loaded_test_loss = checkpoint['test_loss']\n",
    "    \n",
    "    print(f\"Checkpoint carregado com sucesso (Epoch {loaded_epoch})\")\n",
    "    print(f\"Loss de treino no checkpoint: {loaded_loss:.4f}\")\n",
    "    print(f\"Loss de teste no checkpoint: {loaded_test_loss:.4f}\")\n",
    "    \n",
    "    # Coloque o modelo em modo de avaliação\n",
    "    model.eval()\n",
    "else:\n",
    "    print(f\"Erro: Arquivo de checkpoint não encontrado em {checkpoint_path}\")\n",
    "\n",
    "encoder = model.encoder\n",
    "print(next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d30d5f8-27d0-48e2-aa3f-ed1c9e60e128",
   "metadata": {},
   "source": [
    "Create two environments - for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f90ed-4dec-48bb-9e12-f5536072b788",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 49,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1732086835912,
    "lastExecutedByKernel": "773dcfbd-4203-4f1d-ae2a-2fdaf93cdf9b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "env_train = gym.make('CartPole-v1')\nenv_test = gym.make('CartPole-v1')\n",
    "outputsMetadata": {
     "0": {
      "height": 311,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "env_train = suite.load(domain_name=\"cartpole\", task_name=\"swingup\")\n",
    "env_train = pixels.Wrapper(env_train, pixels_only=True,\n",
    "                    render_kwargs={'height': 84, 'width': 84, 'camera_id': 0})\n",
    "# Teste com imagens do ambiente\n",
    "env_test = suite.load(domain_name=\"cartpole\", task_name=\"swingup\")\n",
    "env_test = pixels.Wrapper(env_train, pixels_only=True,\n",
    "                    render_kwargs={'height': 84, 'width': 84, 'camera_id': 0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89a2b58-4d78-4cd5-a7e9-3129498b27df",
   "metadata": {},
   "source": [
    "Create the backbone network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e470fed-6a9b-457d-ab26-6c0dd9b9a429",
   "metadata": {},
   "source": [
    "Define the actor-critic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7604ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dimensions, out_features, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(in_features, hidden_dimensions)\n",
    "        self.layer2 = nn.Linear(hidden_dimensions, hidden_dimensions)\n",
    "        self.layer3 = nn.Linear(hidden_dimensions, out_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = f.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = f.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dimensions, out_features, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(in_features, hidden_dimensions)\n",
    "        self.layer2 = nn.Linear(hidden_dimensions, hidden_dimensions)\n",
    "        self.layer3 = nn.Linear(hidden_dimensions, out_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.Softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = f.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = f.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        action_prob = self.Softmax(x)\n",
    "        dist = distributions.Categorical(action_prob)\n",
    "        action = dist.sample()\n",
    "        log_prob_action = dist.log_prob(action)\n",
    "        return action, log_prob_action, dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fea18d-77bb-410a-a8c0-f6df4b815cbd",
   "metadata": {},
   "source": [
    "Create an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent(HIDDEN_DIMENSIONS, DROPOUT, INPUT_FEATURES, ACTOR_OUTPUT_FEATURES):\n",
    "    CRITIC_OUTPUT_FEATURES = 1\n",
    "\n",
    "    actor = Actor(\n",
    "            INPUT_FEATURES, HIDDEN_DIMENSIONS, ACTOR_OUTPUT_FEATURES, DROPOUT).to(device)\n",
    "    critic = Critic(\n",
    "            INPUT_FEATURES, HIDDEN_DIMENSIONS, CRITIC_OUTPUT_FEATURES, DROPOUT).to(device)\n",
    "    return actor, critic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60d7368-c7ba-4a66-abe5-ee35125ff12c",
   "metadata": {},
   "source": [
    "Define a function to calculate the returns from the rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d45ac0-f393-4375-94a5-a2a7c3cbeeee",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1732086836115,
    "lastExecutedByKernel": "773dcfbd-4203-4f1d-ae2a-2fdaf93cdf9b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def calculate_returns(rewards, discount_factor):\n    returns = []\n    cumulative_reward = 0\n    for r in reversed(rewards):\n        cumulative_reward = r + cumulative_reward * discount_factor\n        returns.insert(0, cumulative_reward)\n\n    returns = torch.tensor(returns)\n    # normalize the return\n    returns = (returns - returns.mean()) / returns.std()\n\n    return returns\n"
   },
   "outputs": [],
   "source": [
    "def calculate_returns(rewards, discount_factor):\n",
    "    returns = []\n",
    "    cumulative_reward = 0\n",
    "    for r in reversed(rewards):\n",
    "        cumulative_reward = r + cumulative_reward * discount_factor\n",
    "        returns.insert(0, cumulative_reward)\n",
    "\n",
    "    returns = torch.tensor(returns)\n",
    "    # normalize the return\n",
    "    returns = (returns - returns.mean()) / returns.std()\n",
    "\n",
    "    return returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4256a3b-a7cd-463b-90f3-df30e8ea6dfb",
   "metadata": {},
   "source": [
    "Define a function to calculate the advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1025d-ddbf-4e5c-9c46-6513e906c268",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1732086836163,
    "lastExecutedByKernel": "773dcfbd-4203-4f1d-ae2a-2fdaf93cdf9b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def calculate_advantages(returns, values):\n    advantages = returns - values\n    # Normalize the advantage\n    advantages = (advantages - advantages.mean()) / advantages.std()\n    return advantages\n"
   },
   "outputs": [],
   "source": [
    "def calculate_advantages(returns, values):\n",
    "    advantages = returns - values\n",
    "    # Normalize the advantage\n",
    "    advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "    return advantages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b5ea26-3cfd-42b4-b38e-47164987ccc5",
   "metadata": {},
   "source": [
    "Define a function to calculate surrogate losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de81c1-6fde-4eb6-a0b9-851709271c24",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1732086836211,
    "lastExecutedByKernel": "773dcfbd-4203-4f1d-ae2a-2fdaf93cdf9b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def calculate_surrogate_loss(\n        actions_log_probability_old,\n        actions_log_probability_new,\n        epsilon,\n        advantages):\n    advantages = advantages.detach()\n    policy_ratio = (\n            actions_log_probability_new - actions_log_probability_old\n            ).exp()\n    surrogate_loss_1 = policy_ratio * advantages\n    surrogate_loss_2 = torch.clamp(\n            policy_ratio, min=1.0-epsilon, max=1.0+epsilon\n            ) * advantages\n    surrogate_loss = torch.min(surrogate_loss_1, surrogate_loss_2)\n    return surrogate_loss\n"
   },
   "outputs": [],
   "source": [
    "def calculate_surrogate_loss(\n",
    "        actions_log_probability_old,\n",
    "        actions_log_probability_new,\n",
    "        epsilon,\n",
    "        advantages):\n",
    "    advantages = advantages.detach()\n",
    "    policy_ratio = (\n",
    "            actions_log_probability_new - actions_log_probability_old\n",
    "            ).exp()\n",
    "    surrogate_loss_1 = policy_ratio * advantages\n",
    "    surrogate_loss_2 = torch.clamp(\n",
    "            policy_ratio, min=1.0-epsilon, max=1.0+epsilon\n",
    "            ) * advantages\n",
    "    surrogate_loss = torch.min(surrogate_loss_1, surrogate_loss_2)\n",
    "    return surrogate_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ea994-289b-4752-8a52-25efff20619d",
   "metadata": {},
   "source": [
    "Define a function to calculate policy loss and value loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40899d67-c62d-4758-819e-9b66bc218216",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1732086836259,
    "lastExecutedByKernel": "773dcfbd-4203-4f1d-ae2a-2fdaf93cdf9b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def calculate_losses(\n        surrogate_loss, entropy, entropy_coefficient, returns, value_pred):\n    entropy_bonus = entropy_coefficient * entropy\n    policy_loss = -(surrogate_loss + entropy_bonus).sum()\n    value_loss = f.smooth_l1_loss(returns, value_pred).sum()\n    return policy_loss, value_loss\n"
   },
   "outputs": [],
   "source": [
    "def calculate_losses(\n",
    "        surrogate_loss, entropy, entropy_coefficient, returns, value_pred):\n",
    "    entropy_bonus = entropy_coefficient * entropy\n",
    "    policy_loss = -(surrogate_loss + entropy_bonus).sum()\n",
    "    value_loss = f.smooth_l1_loss(returns, value_pred).sum()\n",
    "    return policy_loss, value_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ad19cb-6339-4776-b342-879e5d2ac103",
   "metadata": {},
   "source": [
    "Initialize a set of buffers to use during the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62c000-ac4a-46e9-995c-44c99b65714f",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1732086836307,
    "lastExecutedByKernel": "773dcfbd-4203-4f1d-ae2a-2fdaf93cdf9b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def init_training():\n    states = []\n    actions = []\n    actions_log_probability = []\n    values = []\n    rewards = []\n    done = False\n    episode_reward = 0\n    return states, actions, actions_log_probability, values, rewards, done, episode_reward\n"
   },
   "outputs": [],
   "source": [
    "def init_training():\n",
    "    states = []\n",
    "    actions = []\n",
    "    actions_log_probability = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    return states, actions, actions_log_probability, values, rewards, done, episode_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94daa83-9dee-444e-9d48-29e37d6ec8e4",
   "metadata": {},
   "source": [
    "Define the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56423506-a5fc-4c2d-b4cc-e2e491dd9625",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1732086836359,
    "lastExecutedByKernel": "773dcfbd-4203-4f1d-ae2a-2fdaf93cdf9b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def forward_pass(env, agent, optimizer, discount_factor):\n    states, actions, actions_log_probability, values, rewards, done, episode_reward = init_training()\n    state = env.reset()\n    agent.train()\n    while not done:\n        state = torch.FloatTensor(state).unsqueeze(0)\n        states.append(state)\n\n        action_pred, value_pred = agent(state)\n        action_prob = f.softmax(action_pred, dim=-1)\n        dist = distributions.Categorical(action_prob)\n        action = dist.sample()\n        log_prob_action = dist.log_prob(action)\n        state, reward, done, _ = env.step(action.item())\n\n        actions.append(action)\n        actions_log_probability.append(log_prob_action)\n        values.append(value_pred)\n        rewards.append(reward)\n        episode_reward += reward\n\n    states = torch.cat(states)\n    actions = torch.cat(actions)\n    actions_log_probability = torch.cat(actions_log_probability)\n    values = torch.cat(values).squeeze(-1)\n    returns = calculate_returns(rewards, discount_factor)\n    advantages = calculate_advantages(returns, values)\n\n    return episode_reward, states, actions, actions_log_probability, advantages, returns\n"
   },
   "outputs": [],
   "source": [
    "def forward_pass(env, actor, critic, encoder, discount_factor):\n",
    "    states, actions, actions_log_probability, values, rewards, done, episode_reward = init_training()\n",
    "    time_step = env.reset()\n",
    "\n",
    "    # Processamento do estado inicial com float32\n",
    "    state = converter_cinza(time_step.observation['pixels'])\n",
    "    state = state.astype(np.float32) / 127.5 - 1.0  # Já garante float32\n",
    "    state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "    state = state.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_mean, z_log_var = encoder(state)\n",
    "\n",
    "    state = reparameterize(z_mean, z_log_var).detach()\n",
    "\n",
    "    actor.train()\n",
    "    critic.train()\n",
    "    while not done:\n",
    "        states.append(state.detach())\n",
    "        \n",
    "        # Garante float32 nas saídas para trabalhar com mac\n",
    "        action, log_prob_action, _ = actor(state.detach())\n",
    "        value_pred = critic(state).float() \n",
    "        \n",
    "        time_step = env.step(action.item())\n",
    "        done = time_step.last()\n",
    "        reward = time_step.reward if time_step.reward is not None else 0.0\n",
    "        \n",
    "        # Processamento do novo estado\n",
    "        state = converter_cinza(time_step.observation['pixels'])\n",
    "        state = state.astype(np.float32) / 127.5 - 1.0\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        state = state.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z_mean, z_log_var = encoder(state)\n",
    "        state = reparameterize(z_mean, z_log_var)\n",
    "\n",
    "        actions.append(action)\n",
    "        actions_log_probability.append(log_prob_action)\n",
    "        values.append(value_pred)\n",
    "        rewards.append(float(reward))  # Garante Python float\n",
    "        episode_reward += reward\n",
    "\n",
    "    # Concatenação com float32\n",
    "    states = torch.cat(states).to(device)\n",
    "    actions = torch.cat(actions).to(device)\n",
    "    actions_log_probability = torch.cat(actions_log_probability).to(device)\n",
    "    values = torch.cat(values).squeeze(-1).to(device)\n",
    "    \n",
    "    # Modificação crítica: Garante float32 nos retornos\n",
    "    returns = calculate_returns(rewards, discount_factor).float().to(device)  # <-- Correção aqui\n",
    "    advantages = calculate_advantages(returns, values).float().to(device)\n",
    "\n",
    "    return episode_reward, states, actions, actions_log_probability, advantages, returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cbf16b-ad0f-45b7-871b-0048bfa14562",
   "metadata": {},
   "source": [
    "Define the function to update the policy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2f992b-419f-41b3-9f4b-da2426066d89",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 56,
    "lastExecutedAt": 1732086836415,
    "lastExecutedByKernel": "773dcfbd-4203-4f1d-ae2a-2fdaf93cdf9b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def update_policy(\n        agent,\n        states,\n        actions,\n        actions_log_probability_old,\n        advantages,\n        returns,\n        optimizer,\n        ppo_steps,\n        epsilon,\n        entropy_coefficient):\n\n    BATCH_SIZE = 128\n    total_policy_loss = 0\n    total_value_loss = 0\n    actions_log_probability_old = actions_log_probability_old.detach()\n    actions = actions.detach()\n\n    training_results_dataset = TensorDataset(\n            states,\n            actions,\n            actions_log_probability_old,\n            advantages,\n            returns)\n\n    batch_dataset = DataLoader(\n            training_results_dataset,\n            batch_size=BATCH_SIZE,\n            shuffle=False)\n\n    for _ in range(ppo_steps):\n        for batch_idx, (states, actions, actions_log_probability_old, advantages, returns) in enumerate(batch_dataset):\n            # get new log prob of actions for all input states\n            action_pred, value_pred = agent(states)\n            value_pred = value_pred.squeeze(-1)\n            action_prob = f.softmax(action_pred, dim=-1)\n            probability_distribution_new = distributions.Categorical(\n                    action_prob)\n            entropy = probability_distribution_new.entropy()\n\n            # estimate new log probabilities using old actions\n            actions_log_probability_new = probability_distribution_new.log_prob(actions)\n            surrogate_loss = calculate_surrogate_loss(\n                    actions_log_probability_old,\n                    actions_log_probability_new,\n                    epsilon,\n                    advantages)\n            policy_loss, value_loss = calculate_losses(\n                    surrogate_loss,\n                    entropy,\n                    entropy_coefficient,\n                    returns,\n                    value_pred)\n\n            optimizer.zero_grad()\n            policy_loss.backward()\n            value_loss.backward()\n            optimizer.step()\n\n            total_policy_loss += policy_loss.item()\n            total_value_loss += value_loss.item()\n\n    return total_policy_loss / ppo_steps, total_value_loss / ppo_steps\n"
   },
   "outputs": [],
   "source": [
    "def update_policy(\n",
    "        actor,\n",
    "        critic,\n",
    "        states,\n",
    "        actions,\n",
    "        actions_log_probability_old,\n",
    "        advantages,\n",
    "        returns,\n",
    "        optimizer_actor,\n",
    "        optimizer_critic,\n",
    "        ppo_steps,\n",
    "        epsilon,\n",
    "        entropy_coefficient):\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    total_policy_loss = 0\n",
    "    total_value_loss = 0\n",
    "    actions_log_probability_old = actions_log_probability_old.detach()\n",
    "    actions = actions.detach()\n",
    "\n",
    "    training_results_dataset = TensorDataset(\n",
    "            states,\n",
    "            actions,\n",
    "            actions_log_probability_old,\n",
    "            advantages,\n",
    "            returns)\n",
    "\n",
    "    batch_dataset = DataLoader(\n",
    "            training_results_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False)\n",
    "\n",
    "    for _ in range(ppo_steps):\n",
    "        for batch_idx, (states, actions, actions_log_probability_old, advantages, returns) in enumerate(batch_dataset):\n",
    "            # get new log prob of actions for all input states\n",
    "            action, actions_log_probability_new, probability_distribution_new = actor(states) \n",
    "            value_pred = critic(states)\n",
    "            value_pred = value_pred.squeeze(-1)\n",
    "            entropy = probability_distribution_new.entropy()\n",
    "\n",
    "            # estimate new log probabilities using old actions\n",
    "            actions_log_probability_new = probability_distribution_new.log_prob(actions)\n",
    "            surrogate_loss = calculate_surrogate_loss(\n",
    "                    actions_log_probability_old,\n",
    "                    actions_log_probability_new,\n",
    "                    epsilon,\n",
    "                    advantages)\n",
    "            policy_loss, value_loss = calculate_losses(\n",
    "                    surrogate_loss,\n",
    "                    entropy,\n",
    "                    entropy_coefficient,\n",
    "                    returns,\n",
    "                    value_pred)\n",
    "\n",
    "            optimizer_actor.zero_grad() \n",
    "            optimizer_critic.zero_grad() \n",
    "            policy_loss.backward()\n",
    "            value_loss.backward()\n",
    "            optimizer_actor.step()\n",
    "            optimizer_critic.step()\n",
    "            # calculate the total loss\n",
    "            # and add it to the total loss\n",
    "            # for the current batch\n",
    "\n",
    "            total_policy_loss += policy_loss.item()\n",
    "            total_value_loss += value_loss.item()\n",
    "\n",
    "    return total_policy_loss / ppo_steps, total_value_loss / ppo_steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0db6bcb-4456-45aa-a3f4-fb1826316ba5",
   "metadata": {},
   "source": [
    "Define a function to evaluate the model's (policy's) performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299a8bb9-93f7-44de-b154-db836851d23a",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1732086836467,
    "lastExecutedByKernel": "773dcfbd-4203-4f1d-ae2a-2fdaf93cdf9b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def evaluate(env, agent):\n\n    agent.eval()\n    rewards = []\n    done = False\n    episode_reward = 0\n    state = env.reset()\n\n    while not done:\n        state = torch.FloatTensor(state).unsqueeze(0)\n        with torch.no_grad():\n            action_pred, _ = agent(state)\n            action_prob = f.softmax(action_pred, dim=-1)\n        action = torch.argmax(action_prob, dim=-1)\n        state, reward, done, _ = env.step(action.item())\n        episode_reward += reward\n\n    return episode_reward\n"
   },
   "outputs": [],
   "source": [
    "def evaluate(env, actor, encoder, device):\n",
    "    actor.eval()\n",
    "    \n",
    "    time_step = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    # Processamento inicial do estado\n",
    "    state = converter_cinza(time_step.observation['pixels'])\n",
    "    state = state.astype(np.float32) / 127.5 - 1.0\n",
    "    state = torch.tensor(state).float().to(device)  # Convertido para float e movido para o device\n",
    "    state = state.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_mean, z_log_var = encoder(state)\n",
    "    state = reparameterize(z_mean, z_log_var).detach()\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action, log_prob_action, dist = actor(state)\n",
    "\n",
    "        time_step = env.step(action.item())\n",
    "        done = time_step.last()\n",
    "        reward = time_step.reward if time_step.reward is not None else 0.0\n",
    "        \n",
    "        # Processamento do novo estado\n",
    "        state = converter_cinza(time_step.observation['pixels'])\n",
    "        state = state.astype(np.float32) / 127.5 - 1.0\n",
    "        state = torch.tensor(state).float().to(device)  # Convertido para float e movido para o device\n",
    "        state = state.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z_mean, z_log_var = encoder(state)\n",
    "        state = reparameterize(z_mean, z_log_var).detach()\n",
    "        \n",
    "        episode_reward += reward\n",
    "\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2863c94-b105-4a53-9a62-67c5c08c9e9b",
   "metadata": {},
   "source": [
    "Define a function to plot the rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f432433-d155-4d12-a75c-53c0327d11e9",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 55,
    "lastExecutedAt": 1732086836523,
    "lastExecutedByKernel": "773dcfbd-4203-4f1d-ae2a-2fdaf93cdf9b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def plot_train_rewards(train_rewards, reward_threshold):\n    plt.figure(figsize=(12, 8))\n    plt.plot(train_rewards, label='Train Reward')\n    plt.xlabel('Episode', fontsize=20)\n    plt.ylabel('Training Reward', fontsize=20)\n    plt.hlines(reward_threshold, 0, len(train_rewards), color='y')\n    plt.legend(loc='lower right')\n    plt.grid()\n    plt.show()\n"
   },
   "outputs": [],
   "source": [
    "def plot_train_rewards(train_rewards, reward_threshold):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(train_rewards, label='Train Reward')\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Training Reward', fontsize=20)\n",
    "    plt.hlines(reward_threshold, 0, len(train_rewards), color='y')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e515f097-f3ca-4e30-a3be-7fe7fd027c2e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1732086836575,
    "lastExecutedByKernel": "773dcfbd-4203-4f1d-ae2a-2fdaf93cdf9b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def plot_test_rewards(test_rewards, reward_threshold):\n    plt.figure(figsize=(12, 8))\n    plt.plot(test_rewards, label='Test Reward')\n    plt.xlabel('Episode', fontsize=20)\n    plt.ylabel('Testing Reward', fontsize=20)\n    plt.hlines(reward_threshold, 0, len(test_rewards), color='y')\n    plt.legend(loc='lower right')\n    plt.grid()\n    plt.show()\n"
   },
   "outputs": [],
   "source": [
    "def plot_test_rewards(test_rewards, reward_threshold):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(test_rewards, label='Test Reward')\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Testing Reward', fontsize=20)\n",
    "    plt.hlines(reward_threshold, 0, len(test_rewards), color='y')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03367b0-8c43-4ff0-ba6e-fa1d02981923",
   "metadata": {},
   "source": [
    "Define a function to plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e0926a-5df2-4529-9e62-6000027cbff6",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1732086836627,
    "lastExecutedByKernel": "773dcfbd-4203-4f1d-ae2a-2fdaf93cdf9b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def plot_losses(policy_losses, value_losses):\n    plt.figure(figsize=(12, 8))\n    plt.plot(value_losses, label='Value Losses')\n    plt.plot(policy_losses, label='Policy Losses')\n    plt.xlabel('Episode', fontsize=20)\n    plt.ylabel('Loss', fontsize=20)\n    plt.legend(loc='lower right')\n    plt.grid()\n    plt.show()\n"
   },
   "outputs": [],
   "source": [
    "def plot_losses(policy_losses, value_losses):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(value_losses, label='Value Losses')\n",
    "    plt.plot(policy_losses, label='Policy Losses')\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Loss', fontsize=20)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3773d56-c089-420d-bd88-1402693341d8",
   "metadata": {},
   "source": [
    "Define the main function to train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ed6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(): #TODO\n",
    "    # Defina os parâmetros que você deseja testar\n",
    "    # Exemplo: hidden_dimensions, dropout, etc.\n",
    "    #lr = \n",
    "    \n",
    "    numero_aleatorio = random.uniform(1.0, 10.0)\n",
    "    print(numero_aleatorio)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a2fe0-8097-4958-b4a0-d2a3b86e4398",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 59,
    "lastExecutedAt": 1732086836687,
    "lastExecutedByKernel": "773dcfbd-4203-4f1d-ae2a-2fdaf93cdf9b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def run_ppo():\n    MAX_EPISODES = 500\n    DISCOUNT_FACTOR = 0.99\n    REWARD_THRESHOLD = 475\n    PRINT_INTERVAL = 10\n    PPO_STEPS = 8\n    N_TRIALS = 100\n    EPSILON = 0.2\n    ENTROPY_COEFFICIENT = 0.01\n\n    HIDDEN_DIMENSIONS = 64\n    DROPOUT = 0.2\n    LEARNING_RATE = 0.001\n\n    train_rewards = []\n    test_rewards = []\n    policy_losses = []\n    value_losses = []\n\n    agent = create_agent(HIDDEN_DIMENSIONS, DROPOUT)\n    optimizer = optim.Adam(agent.parameters(), lr=LEARNING_RATE)\n\n    for episode in range(1, MAX_EPISODES+1):\n        train_reward, states, actions, actions_log_probability, advantages, returns = forward_pass(\n                env_train,\n                agent,\n                optimizer,\n                DISCOUNT_FACTOR)\n        policy_loss, value_loss = update_policy(\n                agent,\n                states,\n                actions,\n                actions_log_probability,\n                advantages,\n                returns,\n                optimizer,\n                PPO_STEPS,\n                EPSILON,\n                ENTROPY_COEFFICIENT)\n        test_reward = evaluate(env_test, agent)\n\n        policy_losses.append(policy_loss)\n        value_losses.append(value_loss)\n        train_rewards.append(train_reward)\n        test_rewards.append(test_reward)\n\n        mean_train_rewards = np.mean(train_rewards[-N_TRIALS:])\n        mean_test_rewards = np.mean(test_rewards[-N_TRIALS:])\n        mean_abs_policy_loss = np.mean(np.abs(policy_losses[-N_TRIALS:]))\n        mean_abs_value_loss = np.mean(np.abs(value_losses[-N_TRIALS:]))\n\n        if episode % PRINT_INTERVAL == 0:\n            print(f'Episode: {episode:3} | \\\n                  Mean Train Rewards: {mean_train_rewards:3.1f} \\\n                  | Mean Test Rewards: {mean_test_rewards:3.1f} \\\n                  | Mean Abs Policy Loss: {mean_abs_policy_loss:2.2f} \\\n                  | Mean Abs Value Loss: {mean_abs_value_loss:2.2f}')\n\n        if mean_test_rewards >= REWARD_THRESHOLD:\n            print(f'Reached reward threshold in {episode} episodes')\n            break\n\n    plot_train_rewards(train_rewards, REWARD_THRESHOLD)\n    plot_test_rewards(test_rewards, REWARD_THRESHOLD)\n    plot_losses(policy_losses, value_losses)  \n"
   },
   "outputs": [],
   "source": [
    "def run_ppo():\n",
    "    device =  training_device()\n",
    "    LATENT_DIM = 128\n",
    "    IN_CHANNELS = 1\n",
    "    HIDDEN_UNITS = 128\n",
    "    INPUT_FEATURES = LATENT_DIM\n",
    "    ACTOR_OUTPUT_FEATURES = 2 \n",
    "    MAX_EPISODES = 1000\n",
    "    DISCOUNT_FACTOR = 0.99\n",
    "    REWARD_THRESHOLD = 500\n",
    "    PRINT_INTERVAL = 10 #TODO\n",
    "    PPO_STEPS = 10\n",
    "    N_TRIALS = 100\n",
    "    EPSILON = 0.2\n",
    "    ENTROPY_COEFFICIENT = 0.01\n",
    "\n",
    "    HIDDEN_DIMENSIONS = 300\n",
    "    DROPOUT = 0.2\n",
    "    LEARNING_RATE_ACTOR = 0.001\n",
    "    LEARNING_RATE_CRITIC = 0.001\n",
    "\n",
    "    train_rewards = []\n",
    "    test_rewards = []\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "\n",
    "    actor, critic = create_agent(HIDDEN_DIMENSIONS, DROPOUT, INPUT_FEATURES, ACTOR_OUTPUT_FEATURES)\n",
    "    optimizer_actor = optim.Adam(actor.parameters(), lr=LEARNING_RATE_ACTOR)\n",
    "    optimizer_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE_CRITIC)\n",
    "\n",
    "    for episode in range(1, MAX_EPISODES+1):\n",
    "        train_reward, states, actions, actions_log_probability, advantages, returns = forward_pass(\n",
    "                env_train,\n",
    "                actor,\n",
    "                critic,\n",
    "                encoder,\n",
    "                DISCOUNT_FACTOR)\n",
    "        policy_loss, value_loss = update_policy(\n",
    "                actor,\n",
    "                critic,\n",
    "                states,\n",
    "                actions,\n",
    "                actions_log_probability,\n",
    "                advantages,\n",
    "                returns,\n",
    "                optimizer_actor,\n",
    "                optimizer_critic,\n",
    "                PPO_STEPS,\n",
    "                EPSILON,\n",
    "                ENTROPY_COEFFICIENT)\n",
    "        test_reward = evaluate(env_test, actor, encoder, device)\n",
    "\n",
    "        policy_losses.append(policy_loss)\n",
    "        value_losses.append(value_loss)\n",
    "        train_rewards.append(train_reward)\n",
    "        test_rewards.append(test_reward)\n",
    "\n",
    "        mean_train_rewards = np.mean(train_rewards[-N_TRIALS:])\n",
    "        mean_test_rewards = np.mean(test_rewards[-N_TRIALS:])\n",
    "        mean_abs_policy_loss = np.mean(policy_losses[-N_TRIALS:])\n",
    "        mean_abs_value_loss = np.mean(value_losses[-N_TRIALS:])\n",
    "\n",
    "        if episode % PRINT_INTERVAL == 0:\n",
    "            print(f'Episode: {episode:3d} | Training Reward: {train_reward:.1f} | '\n",
    "                  f'Testing Reward: {test_reward:.1f} | '\n",
    "                  f'Mean Train Rewards: {mean_train_rewards:.1f} | '\n",
    "                  f'Mean Test Rewards: {mean_test_rewards:.1f} | '\n",
    "                  f'Mean  Policy Loss: {mean_abs_policy_loss:.2f} | '\n",
    "                  f'Mean  Value Loss: {mean_abs_value_loss:.2f} | '\n",
    "                  f' Policy Loss: {policy_loss:.2f} | '\n",
    "                  f'Value Loss: {value_loss:.2f} | ')\n",
    "\n",
    "    plot_train_rewards(train_rewards, REWARD_THRESHOLD)\n",
    "    plot_test_rewards(test_rewards, REWARD_THRESHOLD)\n",
    "    plot_losses(policy_losses, value_losses)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886f4b69-cb06-4382-b71d-ff374c699927",
   "metadata": {},
   "source": [
    "Run the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c964da7-a62e-4bcc-8a87-18b5d19fcaa0",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 131402,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1732086968090,
    "lastExecutedByKernel": "773dcfbd-4203-4f1d-ae2a-2fdaf93cdf9b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "run_ppo()",
    "outputsMetadata": {
     "0": {
      "height": 122,
      "type": "stream"
     },
     "1": {
      "height": 616,
      "type": "stream"
     },
     "2": {
      "height": 616,
      "type": "stream"
     },
     "3": {
      "height": 101,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "run_ppo() "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ppo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
